# ğŸ¦¾ Offline AI Assistant

This is a lightweight, offline AI assistant for Linux desktops. It can engage in natural conversation, launch applications, run scripts, and is fully customizable. It uses the **Mistral 7B Q5 model** locally and runs entirely on CPU. âš¡

---

## ğŸŒŸ Features

* ğŸ¤– Friendly conversational AI powered by **Mistral 7B**.
* ğŸ–¥ï¸ Launch Linux applications using keywords.
* ğŸ› ï¸ Run scripts like cache clearing, file operations, etc.
* ğŸ“‚ Customizable app/script launcher with JSON configuration.
* ğŸ”’ Fully offline, CPU-based operation.
* ğŸŒ Simple web interface built with **Flask, HTML, CSS, and JS**.

---

## ğŸ“ Repository Structure

```
offline-ai-assistant/
â”œâ”€â”€ programs/
â”‚   â””â”€â”€ linux_apps/
â”‚       â”œâ”€â”€ launcher.py
â”‚       â””â”€â”€ apps.json
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ main.js
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model-mistral  # placeholder, download the model here
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ environment.yml
â”œâ”€â”€ packages.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/gauthamdv/offline-ai-assistant.git
cd offline-ai-assistant
```

### 2ï¸âƒ£ Set up Conda environment (recommended)

```bash
conda env create -f environment.yml
conda activate assistant
```

Alternatively, install with pip:

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Install system packages (Linux)

```bash
xargs sudo apt install -y < packages.txt
```

---

## ğŸ§  Model Setup

1. Navigate to the `models` folder:

```bash
cd models
```

2. Download the **Mistral 7B Q5 model**:

```bash
wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf
```

3. Replace the placeholder file (`model-mistral`) with the downloaded model:

```bash
mv mistral-7b-instruct-v0.2.Q5_K_M.gguf model-mistral
```

> Make sure `app.py` points to the correct model path in the `Llama()` constructor. ğŸ—‚ï¸

---

## ğŸš€ Running the Assistant

1. Activate the conda environment:

```bash
conda activate assistant
```

2. Start the Flask server:

```bash
python app.py
```

3. Open your browser and go to:

```
http://127.0.0.1:5000/
```

---

## ğŸ’¬ Usage

* **Chat**: Type your messages and get responses from the AI.
* **Launch Apps**: Use keywords like `launch browser` or `launch text-editor`.
* **Run Scripts**: Use keywords like `run clean cache` or `run <script-name>` defined in `apps.json`.
* **Reset Conversation**: Click the "Reset" button to clear conversation history.

### ğŸ“ apps.json Example

```json
{
    "text-editor": {
        "aliases": ["notepad", "editor", "text editor"],
        "command": "./programs/linux_apps/launch-text-editor.sh"
    },
    "browser": {
        "aliases": ["firefox", "web", "internet"],
        "command": "./programs/linux_apps/launch-browser.sh"
    },
    "clean-cache": {
        "aliases": ["clean cache", "clear cache"],
        "command": "./programs/clean-cache.sh"
    }
}
```

---

## âš ï¸ Notes

* ğŸ”’ **Offline Only**: Runs entirely offline; no API keys needed.
* ğŸ–¥ï¸ **CPU Usage**: The model runs on CPU; your Ryzen 9 5900HX should handle normal conversation well.
* ğŸŒ± **Future Enhancements**:

  * â± Timer, calculator, and additional scripts.
  * ğŸ§© Better intent detection using the LLM.
  * â• More apps/scripts in `apps.json`.

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a branch: `git checkout -b feature/my-feature`
3. Commit your changes: `git commit -m "Add some feature"`
4. Push to the branch: `git push origin feature/my-feature`
5. Open a Pull Request

---

## ğŸ“¬ Contact

Created by **Gautham DV**. âœ¨

---

## ğŸ—‚ Flowchart

```text
User Input
   â”‚
   â”œâ”€â”€ "launch <app>" â†’ Launcher module â†’ Run app â†’ Response: "<app> launched successfully!"
   â”‚
   â”œâ”€â”€ "run <script>" â†’ Launcher module â†’ Run script â†’ Response: "<script> executed successfully!"
   â”‚
   â””â”€â”€ Else â†’ LLM Response â†’ Generate conversational AI reply â†’ Display in chat
```
